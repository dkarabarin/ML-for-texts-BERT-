# Классификация токсичных комментариев для интернет-магазина "Викишоп"
# Описание проекта
Интернет-магазин «Викишоп» запускает новый сервис, где пользователи могут редактировать и дополнять описания товаров, аналогично вики-сообществам. Клиенты предлагают свои правки и комментируют изменения других.
Задача: Разработать инструмент, который будет автоматически обнаруживать токсичные комментарии и отправлять их на модерацию.
Цель: Обучить модель классифицировать комментарии на позитивные и негативные со значением метрики F1 не менее 0.75.

# Данные
В распоряжении набор данных с разметкой о токсичности правок:
Файл: toxic_comments.csv
Размер датасета: 159,292 комментария
Столбцы:
text - текст комментария
toxic - целевой признак (0 - нетоксичный, 1 - токсичный)
Распределение классов:
Нетоксичные комментарии: 143,106 (89.8%)
Токсичные комментарии: 16,186 (10.2%)

# Структура проекта
text
├── toxic_comments_classification.ipynb  # Основной ноутбук с анализом и обучением
├── requirements.txt                     # Зависимости проекта
├── README.md                           # Документация
└── models/                             # Сохраненные модели (опционально)
# Установка и запуск
Клонируйте репозиторий:
bash
git clone https://github.com/your-username/toxic-comments-classification.git
cd toxic-comments-classification
Установите зависимости:
bash
pip install -r requirements.txt
Запустите ноутбук:
bash
jupyter notebook toxic_comments_classification.ipynb

# Методология
Предобработка данных
Очистка текста: приведение к нижнему регистру, удаление специальных символов
Балансировка: работа с дисбалансом классов через веса классов
Выборка: для ускорения экспериментов использована выборка 1000 комментариев
Извлечение признаков
Модель эмбеддингов: Toxic-BERT (unitary/toxic-bert)
Размерность эмбеддингов: 768 features
Токенизация: максимальная длина 128 токенов

# Модели машинного обучения
Протестированы три классические модели с подбором гиперпараметров:
Логистическая регрессия
Случайный лес
Метод опорных векторов (SVM)

# Валидация и оценка
Кросс-валидация: 5-fold для Logistic Regression, 3-fold для остальных
Метрика: F1-score (основная), precision, recall
Стратегия: Stratified split для сохранения распределения классов

# Результаты
Сравнение моделей на кросс-валидации
Модель	Лучший F1-score (CV)	Лучшие параметры
Logistic Regression	0.9700	{'C': 0.01, 'class_weight': None, 'solver': 'liblinear'}
Random Forest	0.9584	{'class_weight': 'balanced', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200}
SVM	0.9597	{'C': 10, 'class_weight': None, 'gamma': 'scale', 'kernel': 'rbf'}
Финальные результаты на тестовой выборке
Лучшая модель: Logistic Regression
F1-score на тесте: 0.8947
Требование выполнено: ✅ (F1 ≥ 0.75)

# Примеры предсказаний модели
text
1. "This is a great article, thanks for sharing!" → НЕ ТОКСИЧНЫЙ (0.001)
2. "You are so stupid and worthless, I hate you!" → ТОКСИЧНЫЙ (0.988)
3. "I completely disagree with your opinion..." → НЕ ТОКСИЧНЫЙ (0.001)
4. "Go die in a hole, nobody wants you here!" → ТОКСИЧНЫЙ (0.950)
# Ключевые особенности решения
Использование Toxic-BERT
Специализированная модель, предобученная на задаче детекции токсичности
Эффективное извлечение контекстуальных признаков
Высокое качество даже на небольшой выборке
# Обработка дисбаланса классов
Применение весов классов в процессе обучения
Стратифицированное разделение данных
Использование метрики F1 вместо accuracy
Оптимизация производительности
Работа с уменьшенной выборкой для быстрого прототипирования
Эффективная векторизация через BERT-эмбеддинги
Пакетная обработка предсказаний

# Использование модели
python
def predict_toxicity(text, model, tokenizer, bert_model):
    """
    Предсказание токсичности для нового комментария
    """
    # Очистка и токенизация текста
    cleaned_text = clean_text(text)
    encoded = tokenizer.encode_plus(
        cleaned_text,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    # Получение эмбеддинга
    with torch.no_grad():
        outputs = bert_model(**encoded)
    cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
    
    # Предсказание
    prediction = model.predict(cls_embedding)
    probability = model.predict_proba(cls_embedding)[0, 1]
    
    return prediction[0], probability
# Технические требования
Python 3.8+
PyTorch 2.0.0+
Transformers 4.30.0+
Scikit-learn 1.2+
Память: ≥ 4GB RAM
Время обучения: ~10-15 минут на CPU

# Выводы
Достигнутые результаты
✅ Цель достигнута: F1-score = 0.8947 > 0.75
✅ Эффективное решение: Использование специализированного Toxic-BERT
✅ Баланс качества и скорости: Logistic Regression показала лучшие результаты


Калибровка порога классификации под бизнес-требования

Регулярное обновление модели на новых данных
